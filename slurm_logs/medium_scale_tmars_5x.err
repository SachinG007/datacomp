/var/spool/slurmd/job184378/slurm_script: line 23: module: command not found

CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.
To initialize your shell, run

    $ conda init <SHELL_NAME>

Currently supported shells are:
  - bash
  - fish
  - tcsh
  - xonsh
  - zsh
  - powershell

See 'conda init --help' for more information and options.

IMPORTANT: You may need to close and restart your shell after running 'conda init'.


2023-10-09,12:13:25 | INFO | Running in distributed mode with multiple processes. Device: cuda:1.Process (global: 1, local 1), total 4.
2023-10-09,12:13:25 | INFO | Loaded ViT-B-32 model config.
2023-10-09,12:13:25 | INFO | Running in distributed mode with multiple processes. Device: cuda:0.Process (global: 0, local 0), total 4.
2023-10-09,12:13:25 | INFO | Loaded ViT-B-32 model config.
2023-10-09,12:13:25 | INFO | Running in distributed mode with multiple processes. Device: cuda:3.Process (global: 3, local 3), total 4.
2023-10-09,12:13:25 | INFO | Loaded ViT-B-32 model config.
2023-10-09,12:13:25 | INFO | Running in distributed mode with multiple processes. Device: cuda:2.Process (global: 2, local 2), total 4.
2023-10-09,12:13:25 | INFO | Loaded ViT-B-32 model config.
2023-10-09,12:13:29 | INFO | Model:
2023-10-09,12:13:29 | INFO | CLIP(
  (visual): VisionTransformer(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (9): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (10): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (11): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
2023-10-09,12:13:29 | INFO | Params:
2023-10-09,12:13:29 | INFO |   accum_freq: 1
2023-10-09,12:13:29 | INFO |   aug_cfg: {}
2023-10-09,12:13:29 | INFO |   batch_size: 1024
2023-10-09,12:13:29 | INFO |   beta1: 0.9
2023-10-09,12:13:29 | INFO |   beta2: 0.98
2023-10-09,12:13:29 | INFO |   checkpoint_path: /project_data2/projects/sachingo/datacomp_checkpoints/logs/mediumscale_tmars_5x_20231009_121318/checkpoints
2023-10-09,12:13:29 | INFO |   coca_caption_loss_weight: 2.0
2023-10-09,12:13:29 | INFO |   coca_contrastive_loss_weight: 1.0
2023-10-09,12:13:29 | INFO |   copy_codebase: False
2023-10-09,12:13:29 | INFO |   csv_caption_key: title
2023-10-09,12:13:29 | INFO |   csv_img_key: filepath
2023-10-09,12:13:29 | INFO |   csv_separator: 	
2023-10-09,12:13:29 | INFO |   dataset_resampled: True
2023-10-09,12:13:29 | INFO |   dataset_type: webdataset
2023-10-09,12:13:29 | INFO |   ddp_static_graph: True
2023-10-09,12:13:29 | INFO |   debug: False
2023-10-09,12:13:29 | INFO |   delete_previous_checkpoint: False
2023-10-09,12:13:29 | INFO |   device: cuda:0
2023-10-09,12:13:29 | INFO |   dist_backend: nccl
2023-10-09,12:13:29 | INFO |   dist_url: env://
2023-10-09,12:13:29 | INFO |   distill: False
2023-10-09,12:13:29 | INFO |   distill_model: None
2023-10-09,12:13:29 | INFO |   distill_pretrained: None
2023-10-09,12:13:29 | INFO |   distributed: True
2023-10-09,12:13:29 | INFO |   epochs: 25
2023-10-09,12:13:29 | INFO |   epochs_cooldown: None
2023-10-09,12:13:29 | INFO |   eps: 1e-06
2023-10-09,12:13:29 | INFO |   force_custom_text: False
2023-10-09,12:13:29 | INFO |   force_image_size: None
2023-10-09,12:13:29 | INFO |   force_patch_dropout: None
2023-10-09,12:13:29 | INFO |   force_quick_gelu: False
2023-10-09,12:13:29 | INFO |   gather_with_grad: True
2023-10-09,12:13:29 | INFO |   grad_checkpointing: True
2023-10-09,12:13:29 | INFO |   grad_clip_norm: None
2023-10-09,12:13:29 | INFO |   horovod: False
2023-10-09,12:13:29 | INFO |   image_mean: None
2023-10-09,12:13:29 | INFO |   image_std: None
2023-10-09,12:13:29 | INFO |   imagenet_v2: None
2023-10-09,12:13:29 | INFO |   imagenet_val: None
2023-10-09,12:13:29 | INFO |   local_loss: True
2023-10-09,12:13:29 | INFO |   local_rank: 0
2023-10-09,12:13:29 | INFO |   lock_image: False
2023-10-09,12:13:29 | INFO |   lock_image_freeze_bn_stats: False
2023-10-09,12:13:29 | INFO |   lock_image_unlocked_groups: 0
2023-10-09,12:13:29 | INFO |   lock_text: False
2023-10-09,12:13:29 | INFO |   lock_text_freeze_layer_norm: False
2023-10-09,12:13:29 | INFO |   lock_text_unlocked_layers: 0
2023-10-09,12:13:29 | INFO |   log_every_n_steps: 100
2023-10-09,12:13:29 | INFO |   log_level: 20
2023-10-09,12:13:29 | INFO |   log_local: False
2023-10-09,12:13:29 | INFO |   log_path: /project_data2/projects/sachingo/datacomp_checkpoints/logs/mediumscale_tmars_5x_20231009_121318/out.log
2023-10-09,12:13:29 | INFO |   logs: /project_data2/projects/sachingo/datacomp_checkpoints/logs
2023-10-09,12:13:29 | INFO |   lr: 0.0005
2023-10-09,12:13:29 | INFO |   lr_cooldown_end: 0.0
2023-10-09,12:13:29 | INFO |   lr_cooldown_power: 1.0
2023-10-09,12:13:29 | INFO |   lr_scheduler: cosine
2023-10-09,12:13:29 | INFO |   model: ViT-B-32
2023-10-09,12:13:29 | INFO |   name: mediumscale_tmars_5x_20231009_121318
2023-10-09,12:13:29 | INFO |   no_set_device_rank: False
2023-10-09,12:13:29 | INFO |   precision: amp
2023-10-09,12:13:29 | INFO |   pretrained: 
2023-10-09,12:13:29 | INFO |   pretrained_image: False
2023-10-09,12:13:29 | INFO |   rank: 0
2023-10-09,12:13:29 | INFO |   remote_sync: None
2023-10-09,12:13:29 | INFO |   remote_sync_frequency: 300
2023-10-09,12:13:29 | INFO |   remote_sync_protocol: s3
2023-10-09,12:13:29 | INFO |   report_to: wandb
2023-10-09,12:13:29 | INFO |   resume: /home/sachingo/datacomp/logs/logs/mediumscale_tmars_5x_20231005_011454/checkpoints/epoch_4.pt
2023-10-09,12:13:29 | INFO |   save_frequency: 1
2023-10-09,12:13:29 | INFO |   save_most_recent: False
2023-10-09,12:13:29 | INFO |   seed: 0
2023-10-09,12:13:29 | INFO |   skip_scheduler: False
2023-10-09,12:13:29 | INFO |   tensorboard: False
2023-10-09,12:13:29 | INFO |   tensorboard_path: 
2023-10-09,12:13:29 | INFO |   torchscript: False
2023-10-09,12:13:29 | INFO |   trace: False
2023-10-09,12:13:29 | INFO |   train_data: /project_data/datasets/datanet/resharded_masked_vitb_0p281/{00000000..00002559}.tar
2023-10-09,12:13:29 | INFO |   train_data_upsampling_factors: None
2023-10-09,12:13:29 | INFO |   train_num_samples: 25600000
2023-10-09,12:13:29 | INFO |   use_bn_sync: False
2023-10-09,12:13:29 | INFO |   val_data: None
2023-10-09,12:13:29 | INFO |   val_frequency: 1
2023-10-09,12:13:29 | INFO |   val_num_samples: None
2023-10-09,12:13:29 | INFO |   wandb: True
2023-10-09,12:13:29 | INFO |   wandb_notes: 
2023-10-09,12:13:29 | INFO |   wandb_project_name: datanet
2023-10-09,12:13:29 | INFO |   warmup: 500
2023-10-09,12:13:29 | INFO |   wd: 0.2
2023-10-09,12:13:29 | INFO |   workers: 4
2023-10-09,12:13:29 | INFO |   world_size: 4
2023-10-09,12:13:29 | INFO |   zeroshot_frequency: 2
2023-10-09,12:13:55 | INFO | => resuming checkpoint '/home/sachingo/datacomp/logs/logs/mediumscale_tmars_5x_20231005_011454/checkpoints/epoch_4.pt' (epoch 4)
2023-10-09,12:13:55 | INFO | => resuming checkpoint '/home/sachingo/datacomp/logs/logs/mediumscale_tmars_5x_20231005_011454/checkpoints/epoch_4.pt' (epoch 4)
2023-10-09,12:13:55 | INFO | => resuming checkpoint '/home/sachingo/datacomp/logs/logs/mediumscale_tmars_5x_20231005_011454/checkpoints/epoch_4.pt' (epoch 4)
2023-10-09,12:13:55 | INFO | => resuming checkpoint '/home/sachingo/datacomp/logs/logs/mediumscale_tmars_5x_20231005_011454/checkpoints/epoch_4.pt' (epoch 4)
wandb: Currently logged in as: saching007. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.12 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/sachingo/datacomp/wandb/run-20231009_121355-mediumscale_tmars_5x_20231009_121318
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run mediumscale_tmars_5x_20231009_121318
wandb: ⭐️ View project at https://wandb.ai/saching007/datanet
wandb: 🚀 View run at https://wandb.ai/saching007/datanet/runs/mediumscale_tmars_5x_20231009_121318
[34m[1mwandb[0m: [33mWARNING[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
2023-10-09,12:14:29 | INFO | Start epoch 4
2023-10-09,12:14:43 | INFO | Train Epoch: 4 [    4096/25608192 (0%)] Data (t): 9.171 Batch (t): 14.390, 284.647/s, 71.1617/s/gpu LR: 0.000470 Logit Scale: 48.458 Contrastive_loss: 1.3730 (1.3730) Loss: 1.3730 (1.3730)
2023-10-09,12:14:45 | INFO | Reducer buckets have been rebuilt in this iteration.
2023-10-09,12:14:45 | INFO | Reducer buckets have been rebuilt in this iteration.
2023-10-09,12:14:45 | INFO | Reducer buckets have been rebuilt in this iteration.
2023-10-09,12:14:45 | INFO | Reducer buckets have been rebuilt in this iteration.
2023-10-09,12:17:46 | INFO | Train Epoch: 4 [  413696/25608192 (2%)] Data (t): 0.428 Batch (t): 1.829, 2272.93/s, 568.233/s/gpu LR: 0.000470 Logit Scale: 48.500 Contrastive_loss: 1.6699 (1.5214) Loss: 1.6699 (1.5214)
2023-10-09,12:20:50 | INFO | Train Epoch: 4 [  823296/25608192 (3%)] Data (t): 0.441 Batch (t): 1.840, 2270.08/s, 567.520/s/gpu LR: 0.000470 Logit Scale: 48.365 Contrastive_loss: 1.6549 (1.5659) Loss: 1.6549 (1.5659)
2023-10-09,12:23:51 | INFO | Train Epoch: 4 [ 1232896/25608192 (5%)] Data (t): 0.421 Batch (t): 1.808, 2256.60/s, 564.150/s/gpu LR: 0.000469 Logit Scale: 48.448 Contrastive_loss: 1.3771 (1.5187) Loss: 1.3771 (1.5187)
2023-10-09,12:26:53 | INFO | Train Epoch: 4 [ 1642496/25608192 (6%)] Data (t): 0.430 Batch (t): 1.818, 2272.88/s, 568.220/s/gpu LR: 0.000469 Logit Scale: 48.587 Contrastive_loss: 1.6289 (1.5408) Loss: 1.6289 (1.5408)
2023-10-09,12:29:54 | INFO | Train Epoch: 4 [ 2052096/25608192 (8%)] Data (t): 0.427 Batch (t): 1.819, 2257.02/s, 564.255/s/gpu LR: 0.000469 Logit Scale: 48.555 Contrastive_loss: 1.8016 (1.5842) Loss: 1.8016 (1.5842)
2023-10-09,12:32:55 | INFO | Train Epoch: 4 [ 2461696/25608192 (10%)] Data (t): 0.421 Batch (t): 1.808, 2285.83/s, 571.457/s/gpu LR: 0.000469 Logit Scale: 48.708 Contrastive_loss: 1.6066 (1.5874) Loss: 1.6066 (1.5874)
2023-10-09,12:35:56 | INFO | Train Epoch: 4 [ 2871296/25608192 (11%)] Data (t): 0.421 Batch (t): 1.810, 2164.15/s, 541.038/s/gpu LR: 0.000468 Logit Scale: 48.582 Contrastive_loss: 1.8093 (1.6152) Loss: 1.8093 (1.6152)
2023-10-09,12:38:57 | INFO | Train Epoch: 4 [ 3280896/25608192 (13%)] Data (t): 0.417 Batch (t): 1.805, 2280.18/s, 570.044/s/gpu LR: 0.000468 Logit Scale: 48.658 Contrastive_loss: 1.8482 (1.6411) Loss: 1.8482 (1.6411)
2023-10-09,12:41:57 | INFO | Train Epoch: 4 [ 3690496/25608192 (14%)] Data (t): 0.421 Batch (t): 1.805, 2189.61/s, 547.401/s/gpu LR: 0.000468 Logit Scale: 48.695 Contrastive_loss: 1.6979 (1.6467) Loss: 1.6979 (1.6467)
2023-10-09,12:44:57 | INFO | Train Epoch: 4 [ 4100096/25608192 (16%)] Data (t): 0.420 Batch (t): 1.802, 2297.47/s, 574.368/s/gpu LR: 0.000468 Logit Scale: 48.741 Contrastive_loss: 1.7318 (1.6545) Loss: 1.7318 (1.6545)
2023-10-09,12:47:58 | INFO | Train Epoch: 4 [ 4509696/25608192 (18%)] Data (t): 0.424 Batch (t): 1.806, 2288.88/s, 572.220/s/gpu LR: 0.000467 Logit Scale: 48.879 Contrastive_loss: 1.3213 (1.6267) Loss: 1.3213 (1.6267)
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** STEP 184378.0 ON locus-1-29 CANCELLED AT 2023-10-09T12:50:29 ***
slurmstepd: error: *** JOB 184378 ON locus-1-29 CANCELLED AT 2023-10-09T12:50:29 ***

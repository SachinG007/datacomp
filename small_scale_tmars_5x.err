ModuleCmd_Load.c(213):ERROR:105: Unable to locate a modulefile for 'openmpi'
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Traceback (most recent call last):
Traceback (most recent call last):
  File "/home/sachingo/datacomp/train.py", line 304, in <module>
  File "/home/sachingo/datacomp/train.py", line 304, in <module>
        success = main(main_args)success = main(main_args)

  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/training/main.py", line 82, in main
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/training/main.py", line 82, in main
    device = init_distributed_device(args)
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/training/distributed.py", line 88, in init_distributed_device
    device = init_distributed_device(args)
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/training/distributed.py", line 88, in init_distributed_device
    torch.distributed.init_process_group(
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 761, in init_process_group
    torch.distributed.init_process_group(
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 761, in init_process_group
    default_pg = _new_process_group_helper(
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 897, in _new_process_group_helper
    default_pg = _new_process_group_helper(
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py", line 897, in _new_process_group_helper
    pg = ProcessGroupNCCL(prefix_store, group_rank, group_size, pg_options)
RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
    pg = ProcessGroupNCCL(prefix_store, group_rank, group_size, pg_options)
RuntimeError: ProcessGroupNCCL is only supported with GPUs, no GPUs found!
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 454819) of binary: /home/sachingo/miniconda3/envs/open_clip/bin/python
Traceback (most recent call last):
  File "/home/sachingo/miniconda3/envs/open_clip/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/sachingo/miniconda3/envs/open_clip/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2023-10-03_15:20:37
  host      : locus-1-29.eth
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 454820)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-10-03_15:20:37
  host      : locus-1-29.eth
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 454819)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
2023-10-03,15:20:41 | INFO | No latest resume checkpoint found in logs/smallscale_tmars_5x/checkpoints.
2023-10-03,15:20:41 | INFO | Running with a single process. Device cuda:0.
2023-10-03,15:20:41 | INFO | Loaded ViT-B-32 model config.
2023-10-03,15:20:47 | INFO | Model:
2023-10-03,15:20:47 | INFO | CLIP(
  (visual): VisionTransformer(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (1): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (2): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (3): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (4): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (5): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (6): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (7): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (8): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (9): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (10): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
        (11): ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): ModuleList(
      (0): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (1): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (2): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (3): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (4): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (5): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (6): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (7): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (8): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (9): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (10): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
      (11): ResidualAttentionBlock(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
        (ls_2): Identity()
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
2023-10-03,15:20:47 | INFO | Params:
2023-10-03,15:20:47 | INFO |   accum_freq: 1
2023-10-03,15:20:47 | INFO |   aug_cfg: {}
2023-10-03,15:20:47 | INFO |   batch_size: 4096
2023-10-03,15:20:47 | INFO |   beta1: 0.9
2023-10-03,15:20:47 | INFO |   beta2: 0.98
2023-10-03,15:20:47 | INFO |   checkpoint_path: logs/smallscale_tmars_5x/checkpoints
2023-10-03,15:20:47 | INFO |   coca_caption_loss_weight: 2.0
2023-10-03,15:20:47 | INFO |   coca_contrastive_loss_weight: 1.0
2023-10-03,15:20:47 | INFO |   copy_codebase: False
2023-10-03,15:20:47 | INFO |   csv_caption_key: title
2023-10-03,15:20:47 | INFO |   csv_img_key: filepath
2023-10-03,15:20:47 | INFO |   csv_separator: 	
2023-10-03,15:20:47 | INFO |   dataset_resampled: True
2023-10-03,15:20:47 | INFO |   dataset_type: webdataset
2023-10-03,15:20:47 | INFO |   ddp_static_graph: True
2023-10-03,15:20:47 | INFO |   debug: False
2023-10-03,15:20:47 | INFO |   delete_previous_checkpoint: False
2023-10-03,15:20:47 | INFO |   device: cuda:0
2023-10-03,15:20:47 | INFO |   dist_backend: nccl
2023-10-03,15:20:47 | INFO |   dist_url: env://
2023-10-03,15:20:47 | INFO |   distill: False
2023-10-03,15:20:47 | INFO |   distill_model: None
2023-10-03,15:20:47 | INFO |   distill_pretrained: None
2023-10-03,15:20:47 | INFO |   distributed: False
2023-10-03,15:20:47 | INFO |   epochs: 8
2023-10-03,15:20:47 | INFO |   epochs_cooldown: None
2023-10-03,15:20:47 | INFO |   eps: 1e-06
2023-10-03,15:20:47 | INFO |   force_custom_text: False
2023-10-03,15:20:47 | INFO |   force_image_size: None
2023-10-03,15:20:47 | INFO |   force_patch_dropout: None
2023-10-03,15:20:47 | INFO |   force_quick_gelu: False
2023-10-03,15:20:47 | INFO |   gather_with_grad: True
2023-10-03,15:20:47 | INFO |   grad_checkpointing: True
2023-10-03,15:20:47 | INFO |   grad_clip_norm: None
2023-10-03,15:20:47 | INFO |   horovod: False
2023-10-03,15:20:47 | INFO |   image_mean: None
2023-10-03,15:20:47 | INFO |   image_std: None
2023-10-03,15:20:47 | INFO |   imagenet_v2: None
2023-10-03,15:20:47 | INFO |   imagenet_val: None
2023-10-03,15:20:47 | INFO |   local_loss: True
2023-10-03,15:20:47 | INFO |   local_rank: 0
2023-10-03,15:20:47 | INFO |   lock_image: False
2023-10-03,15:20:47 | INFO |   lock_image_freeze_bn_stats: False
2023-10-03,15:20:47 | INFO |   lock_image_unlocked_groups: 0
2023-10-03,15:20:47 | INFO |   lock_text: False
2023-10-03,15:20:47 | INFO |   lock_text_freeze_layer_norm: False
2023-10-03,15:20:47 | INFO |   lock_text_unlocked_layers: 0
2023-10-03,15:20:47 | INFO |   log_every_n_steps: 100
2023-10-03,15:20:47 | INFO |   log_level: 20
2023-10-03,15:20:47 | INFO |   log_local: False
2023-10-03,15:20:47 | INFO |   log_path: logs/smallscale_tmars_5x/out.log
2023-10-03,15:20:47 | INFO |   logs: logs
2023-10-03,15:20:47 | INFO |   lr: 0.0005
2023-10-03,15:20:47 | INFO |   lr_cooldown_end: 0.0
2023-10-03,15:20:47 | INFO |   lr_cooldown_power: 1.0
2023-10-03,15:20:47 | INFO |   lr_scheduler: cosine
2023-10-03,15:20:47 | INFO |   model: ViT-B-32
2023-10-03,15:20:47 | INFO |   name: smallscale_tmars_5x
2023-10-03,15:20:47 | INFO |   no_set_device_rank: False
2023-10-03,15:20:47 | INFO |   precision: amp
2023-10-03,15:20:47 | INFO |   pretrained: 
2023-10-03,15:20:47 | INFO |   pretrained_image: False
2023-10-03,15:20:47 | INFO |   rank: 0
2023-10-03,15:20:47 | INFO |   remote_sync: None
2023-10-03,15:20:47 | INFO |   remote_sync_frequency: 300
2023-10-03,15:20:47 | INFO |   remote_sync_protocol: s3
2023-10-03,15:20:47 | INFO |   report_to: wandb
2023-10-03,15:20:47 | INFO |   resume: None
2023-10-03,15:20:47 | INFO |   save_frequency: 0
2023-10-03,15:20:47 | INFO |   save_most_recent: True
2023-10-03,15:20:47 | INFO |   seed: 0
2023-10-03,15:20:47 | INFO |   skip_scheduler: False
2023-10-03,15:20:47 | INFO |   tensorboard: False
2023-10-03,15:20:47 | INFO |   tensorboard_path: 
2023-10-03,15:20:47 | INFO |   torchcompile: False
2023-10-03,15:20:47 | INFO |   torchscript: False
2023-10-03,15:20:47 | INFO |   trace: False
2023-10-03,15:20:47 | INFO |   train_data: /project_data/datasets/datanet/small_scale_tmars/shards/{00000000..00000260}.tar
2023-10-03,15:20:47 | INFO |   train_data_upsampling_factors: None
2023-10-03,15:20:47 | INFO |   train_num_samples: 8000000
2023-10-03,15:20:47 | INFO |   use_bn_sync: False
2023-10-03,15:20:47 | INFO |   use_bnb_linear: None
2023-10-03,15:20:47 | INFO |   val_data: None
2023-10-03,15:20:47 | INFO |   val_frequency: 1
2023-10-03,15:20:47 | INFO |   val_num_samples: None
2023-10-03,15:20:47 | INFO |   wandb: True
2023-10-03,15:20:47 | INFO |   wandb_notes: 
2023-10-03,15:20:47 | INFO |   wandb_project_name: datanet
2023-10-03,15:20:47 | INFO |   warmup: 500
2023-10-03,15:20:47 | INFO |   wd: 0.2
2023-10-03,15:20:47 | INFO |   workers: 4
2023-10-03,15:20:47 | INFO |   world_size: 1
2023-10-03,15:20:47 | INFO |   zeroshot_frequency: 2
wandb: Currently logged in as: saching007. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.11 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.21
wandb: Run data is saved locally in /home/sachingo/datacomp/wandb/run-20231003_152047-smallscale_tmars_5x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smallscale_tmars_5x
wandb: ⭐️ View project at https://wandb.ai/saching007/datanet
wandb: 🚀 View run at https://wandb.ai/saching007/datanet/runs/smallscale_tmars_5x
2023-10-03,15:20:55 | INFO | Start epoch 0
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** JOB 183556 ON locus-1-29 CANCELLED AT 2023-10-03T15:21:12 ***
slurmstepd: error: *** STEP 183556.0 ON locus-1-29 CANCELLED AT 2023-10-03T15:21:12 ***
